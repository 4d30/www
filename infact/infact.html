<!DOCTYPE HTML PUBLIC '-//W3C//DTD HTML 4.01 Transitional//EN'>
<html>
<head>
<title>Python, Data Science, Life</title>
</head>

<link rel="stylesheet" type="text/css" href="../style.css" media="screen"/>
<style>
#outer {}
#inner {margin: auto;
	line-height: 1.5em;
	text-align: justify; 
	min-width: 500px; 
	max-width: 1000px;
	border: 1px solid black;
	background-color: #ffffff;
	padding: 5px 5px 5px 5px;
	}


</style>
<!--   the head of the page   -->
<body>
	<center><h1>How to determine which skills to put on a resume</h1></center>
	<div id="inner">
	<img style="float: right; padding: 0px 5px;" src="tools.jpg" title="Photo by J Boontie from FreeImages" width="33%" >
	<p><b>This page is under construction.</b><br><br>
	<b>You have lots of skills, too many to fit on a one-page resume.</b><br>
	&nbspWe develop skills over time, usually lots of them.
	Unfortunately, hiring managers don't have the time, energy, or need to hear about each and every skill you've developed over your lifetime.
	Standard advice for job-seekers is to keep one's resume length fixed at one page.
	When it comes to choosing which skills to list on a resume it is best practice to list the skills which one posesses that are related to the position.
	This step includes reading the job description and identifying skills one has that would support this role.  <br><br>

	<b>OK. I've read the description, reduced my margins, and am using a 10pt font. What else?</b><br>
	&nbspWith the easy steps taken care of, it's time to work for it.
	Our strategy will be to, given corpus a of similar job postings, suggest skills which are not in the description but are on similar listings.
	Here's the process:<br></p>
	<ol>
		<li>Search jobsite for the target role with <a href="https://docs.python-requests.org/en/master/">requests</a></li>
		<li>Parse search results to find URLs of all job description HTML documents with <a href="https://crummy.com/software/BeautifulSoup/bs4/doc/">BeautifulSoup</a></li>
		<li>Download and cache job description HTML</li>
		<li>Extract job description from HTML</li>
		<li>Extract features from job description with <a href="https://www.nltk.org">NLTK</a></li>
		<li>Calculate <a href="https://nlp.stanford.edu/IR-book/html/htmledition/tf-idf-weighting-1.html">tf-idf</a> for each feature</li>
		<li>Filter and sort results, let the information swim to the top</li>
	</ol>
	</div>

	<center>
	<p style="text-align: justify; border: 1px solid black; color: #ffffff; background-color: #000000; padding: 5px 5px 5px 5px; max-width: 750px;">

	
	<code style="background-color: #000000; color:#ffffff;">
		$dict corpus<br>
		&nbsp;&nbsp;From WordNet (r) 3.0 (2006) [wn]:<br>
		&nbsp;&nbsp;&nbsp;corpus<br>
		&nbsp;&nbsp;&nbsp;&nbsp;n 2: a collection of writings; "he edited the Hemingway corpus"</code>
	</p>
	</center>

	<div id="inner"><p>
	<b>Search Jobsite for target role</b><br>
	We're not going to use the web front-end for this.
	That would take too much time.
	Instead we are going to use Python's <i>requests</i> module to navigate.
	Since it is a violation of the terms of service for many jobsites to download any content, I will use a fictitious the jobsite "infact.com" to demonstrate.
	An example URL for a Data Scientist role in New York, NY could look like:</p>
	<center>
	<p style="text-align: justify; color: #444444; background-color: #ffffb0; padding: 5px 5px 5px 5px; max-width: 750px;">
	<code>
		#!/usr/bin/env python3<br>
		ROLE = 'Data Scientist'<br>
		LOCATION = 'New York, NY'<br>
		URL = f'https://www.infact.com/jobs/?q={ROLE}&l={LOCATION}&sort=date'<br>
	</code>
	</p>
	</center>
	<p> Now we use the <a href="https://docs.python-requests.org/en/master/">requests</a> module to fetch the HTML from infact.com's server:</p>
	<center>
	<p style="text-align: justify; color: #444444; background-color: #ffffb0; padding: 5px 5px 5px 5px; max-width: 750px;">
	<code>
		import requests<br>
		response = requests.get(URL)<br>
		HTML = response.text()
	</code>
	</p>
	</center><p>
	Since infact.com is completely fictitious, we'll imagine to have an HTML document which contains the job listings for the role and location queried sorted by date posted.
	Further, we can imagine this data is provided in a systematic way which links each listing to another page with a full job description.
	An excellent way to navigate through HTML tags is with the <a href="https://crummy.com/software/BeautifulSoup/bs4/doc/">BeautifulSoup</a> module.
	Let's make a guess that on infact.com, the job content is loaded dynamically through JavaScript.
	Assume that inside a &lt;script&gt; tag each job-listing is contained inside a dictionary-like object called 'jobmap.' <center> <p style="text-align: justify; color: #444444; background-color: #ffffb0; padding: 5px 5px 5px 5px; max-width: 750px;"> <code> 
		import re&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#for regular expressions<br>
		from bs4 import BeautifulSoup<br>
		soup = BeautifulSoup(HTML, 'html.parser')<br>
		jobmap = soup.find(text=re.compile('jobmap'))
	</code>
	</p>
	</center>
	<p>
	<b>Parse search results to find URLs of all job description HTML documents</b><br>
	The jobmap proably contains lots of data like a unique identifier of the role to be filled, the employer, the date posted, etc..
	Let's call the unique identifier a jobkey or <b>jk</b> for short.
	Although not incredibly challenging, this part can get pretty hacky.
	With some looping, logic, and string functions, I leave it to the reader to extract the jobkeys for each job-posting.
	Be creative :)
	<br><br>
	With a list or dictionary of jobkeys, we can use the <i>requests</i> module to fetch the HTML document which contains the full job-description.
	Let's assume we have a dictionary of the following format:<br>
	<center>
	<code>
	<table style="background-color: #b0b0ff;">
		<tr><td></td><td>jk</td></tr>
		<tr><td>1</td><td>038eef0fed4b586d</td></tr>
		<tr><td>2</td><td>96a3d424eb8855e1</td></tr>
		<tr><td>3</td><td>faf9d0ffb3232d4f</td></tr>
		<tr><td>...</td><td>...</td></tr>
		<tr><td>N</td><td>ffffffffffffffff</td></tr>
		
	</table>
	</code>
	</center>
	<p><b>Download and cache all job description HTML documents</b><br>
	The HTTP-request part will closely the format we used earlier. 
	What is different is new URLs are programatically created for each jobkey. 
	Also the results are being cached on the disk.<br></p>
	<center>
	<p style="max-width: 400;">
	<i>Nota bene: Caching page-content is often a violation of a webpage's TOS. Check in with the host's TOS and with your own morality before caching.</i>
	</p>
	<p style="text-align: justify; color: #444444; background-color: #ffffb0; padding: 5px 5px 5px 5px; max-width: 750px;">
	<code> 
	BASE_URL = 'https://www.infact.com/viewjob?jk='<br>
	for jk in jobmap['jk']:<br>
	&nbsp;&nbsp;&nbsp;filename = './cache/'+jk<br>
	&nbsp;&nbsp;&nbsp;if os.path.isfile(filename):<br>
	&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;continue<br>
	&nbsp;&nbsp;&nbsp;else:<br>
	&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;URL = BASE_URL + jk<br>
	&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;response = request.get(URL)<br>
	&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;with open(filename,'w') as f:<br>
	&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;f.write(response.text)<br>

	</code>
	</center>
	</p>
	<p>
	<b>Process the beautiful content: TF-IDF </b><br>
	Write some prose on tf-idf.<br>
	<br>


	<b>Count raw term frequency</b><br>
	The task is to break down each job-description first into sentences and then into tokens.
	Once we have tokens, stop-words will be filtered out and each term will be lemmatized with it's part of speech.
	Finally, uni-, bi-, and tri-grams will be created from each sentence.
	Of course one could proceed with larger grams, but going much further into gram-space tends to create a sparse dataset.
	NLTK is a great tool for this task and others exist.
	Finally, each processed feature will be counted.
	</p>
	<center>
	<p style="text-align: justify; color: #444444; background-color: #ffffb0; padding: 5px 5px 5px 5px; max-width: 750px;">
	<code> 
	#!/usr/bin/env python3<br>
	# "process_content.py"<br>
	<br>
	from nltk import pos_tag<br>
	from nltk.tokenize import sent_tokenize, RegexpTokenizer<br>
	from nltk.probability import Counter<br>
	from nltk.corpus import stopwords, wordnet<br>
	from nltk.stem import WordNetLemmatizer<br>
	from nltk.util import ngrams<br>
	<br>
	tokenizer = RegexpTokenizer(r'\w+')<br>
	wnl = WordNetLemmatizer()<br>
	term_counter = Counter()<br>	
	<br>
	for file in files:<br>
	&nbsp;&nbsp;&nbsp;raw = file.read().decode('utf8')<br>
	&nbsp;&nbsp;&nbsp;raw = raw.lower()<br>
	&nbsp;&nbsp;&nbsp;sent = tokenize(raw)<br>
	&nbsp;&nbsp;&nbsp;words = tokenizer.tokenize(sent)<br>
	&nbsp;&nbsp;&nbsp;tagged_words = pos_tag(words)<br>
	&nbsp;&nbsp;&nbsp;filtered_words = [t for t in tagged_words if not t[0] in stop_words]<br>
	&nbsp;&nbsp;lemmas = [wnl.lemmatize(w[0], pos=convert_function(w[1])) for w in filtered_words]<br>
	&nbsp;&nbsp;&nbsp;for sentence in lemmas:<br>
	&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for num in range(1,4):<br>
	&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;grams = ngrams(sentence, num)<br>
	&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;term_counter.update(grams)<br>
	[ print(v) for i, v in enumerate(term_counter.items()) if i &lt; 5 ]
	</code>
	</center>
	</p>
	<p>
	The last line of the script prints out a little preview so we can be sure we're on the right track.
	</p>
	</div>

	<center>
	<p style="text-align: justify; border: 1px solid black; color: #ffffff; background-color: #000000; padding: 5px 5px 5px 5px; max-width: 750px;">

	
	<code style="background-color: #000000; color:#ffffff;">
		$./process_content.py<br>
		&nbsp;&nbsp;(('applied',), 32)<br>
		&nbsp;&nbsp;(('commercial',), 42)<br>
		&nbsp;&nbsp;(('artificial','intelligence',), 79)<br>
		&nbsp;&nbsp;(('program',), 247)<br>
		&nbsp;&nbsp;(('equal','opportunity','employer',), 105)<br>
	</p>
	</center>

	<div id="inner"><p>
	<b>Count raw document frequency</b><br>
	We will use what we learned in the <i>tf</i> bit above to help us in the next part.
	Namely, we now have a list of all of the features which appear in the corpus.
	This list will be used to determine which features are in each document.
	It is probably possible to determine document frequency at the same time as raw term frequnecy.
	The added complexity probably isn't worth the efficiency.
	</p>
	<center>
	<p style="text-align: justify; color: #444444; background-color: #ffffb0; padding: 5px 5px 5px 5px; max-width: 750px;">
	<code> 
	# Given an input list of TERMS and a file, this function returns a <br>
	# list of bool for for each term.<br>
	<br>
	from nltk.probability import FreqDist<br>
	<br>
	def which&lowbar;terms&lowbar;in&lowbar;doc(TERMS, FILENAME):<br>
	&nbsp;&nbsp;&nbsp;termsindoc= FreqDist()<br>
	&nbsp;&nbsp;&nbsp;with open(FILENAME, 'rb') as f:<br>
	&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;sent = get_sentences(f)<br>
	&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;lemmas = map(get_lemmas, sent)<br>
	&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;termsindoc.update(get_terms(lemmas))<br>
	&nbsp;&nbsp;&nbsp;return list(map(lambda x: x in in termsindoc.keys(), TERMS))
	</code>
	</center>
	<p>
	There are a few locally defined functions in the snippet above.
	Their function is self-evident and if you're interested in their 
	definitions please refer to my source on 
	<a href="https://github.com/4d30/infact/blob/main/descprocessing.py">GitHub</a>.  </p>
	<p>
	<b>Calculations</b><br>
	There are a few different ways to calculate <i>term frequency</i> and <i>document frequency</i>.
	The simplest is to just divide the number of occurances by the total number of values: terms or documents.
	For the term frequency, 
	<a href="https://nlp.stanford.edu/IR-book/html/htmledition/sublinear-tf-scaling-1.html">Cambridge University</a> 
	suggests a modification:</p>
	<br>
	<img style="display: block; margin-left: auto; margin-right: auto;" src="./wf.png"> <br>	
	<p>
	Document frequency will follow a similar logarithmic treatment:<br>
	<img style="display: block; margin-left: auto; margin-right: auto;" src="./idf.png"> <br>	
	<br>	
	<br>	
	<br>	




	</div>
	<br><br>	
<!--- the body of the page ---!>
<!--   footnotes   -->

</body>
</html>
